# ğŸ›¡ï¸ SOC Dashboard Suite

## Email + Telemetry Threat Monitoring

> A security analytics engineering project that designs and implements a **mini-SOC monitoring system** for email-borne threats and authentication telemetry, focusing on detection reliability, false-positive control, and analyst-ready investigation workflows.

---

## ğŸ“Œ Project Motivation

Modern SOC failures are rarely caused by a lack of data or models.
They fail because of:

* excessive false positives
* weak context enrichment
* alert fatigue
* poor correlation across telemetry
* dashboards optimized for visuals instead of decisions

This project intentionally **does not start with machine learning**.

Instead, it focuses on:

* defining reliable security signals
* designing a defensible detection pipeline
* enforcing evidence-first alerting
* building dashboards that support **triage and investigation**, not vanity metrics

The goal is to simulate how a **real enterprise SOC** designs, evaluates, and operates its monitoring systems.

---

## ğŸ§  Design Philosophy

This project follows the same discipline used in production SOC environments:

* Detection > Classification
* Alerts > Metrics
* Context > Volume
* Explainability > Accuracy
* Governance > Speed
* ML is *supporting*, never central

If a component increases alert noise, reduces analyst trust, or hides reasoning, it is rejected by design.

---

## ğŸ—ï¸ System Overview

The SOC Dashboard Suite models an end-to-end monitoring pipeline:

```
Telemetry Sources
   â†“
Ingestion & Normalization
   â†“
Context Enrichment & Baselining
   â†“
Detection Engineering
   â†“
Alerting & Governance
   â†“
Dashboards & Investigation Views
```

### Telemetry Domains

* **Email telemetry**

  * sender identity
  * authentication signals (SPF/DKIM/DMARC)
  * attachment and URL metadata
* **Authentication telemetry**

  * login successes/failures
  * geographic and temporal context
  * device and identity attributes

---

## ğŸ” What This Project Explicitly Does

âœ” Defines SOC-grade schemas for email and authentication telemetry
âœ” Implements ingestion and normalization with data quality guarantees
âœ” Adds identity, domain, temporal, and baseline context
âœ” Engineers explainable detection logic with tuning knobs
âœ” Controls alert volume through gating, deduplication, and severity scoring
âœ” Produces analyst-centric dashboards for triage and investigation
âœ” Threat-models the SOC system itself
âœ” Maps the architecture to real cloud SIEM platforms

---

## ğŸš« What This Project Intentionally Does NOT Do

âŒ No deep learning models
âŒ No accuracy / F1 / ROC-driven claims
âŒ No feature ranking for its own sake
âŒ No â€œAI SOCâ€ marketing narratives
âŒ No unrealistic assumptions about perfect data

Any machine learning used is **minimal, interpretable, and subordinate** to SOC constraints.

---

## ğŸ“ Repository Structure

```
soc-dashboard-suite/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ soc_system_overview.md
â”‚   â”œâ”€â”€ soc_system_diagram.png
â”‚   â””â”€â”€ threat_model.md
â”‚
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ email_schema.md
â”‚   â”œâ”€â”€ telemetry_schema.md
â”‚   â””â”€â”€ unified_event_schema.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ email/
â”‚   â”‚   â””â”€â”€ telemetry/
â”‚   â”œâ”€â”€ normalized/
â”‚   â””â”€â”€ enriched/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ email_ingestion.ipynb
â”‚   â””â”€â”€ telemetry_ingestion.ipynb
â”‚
â”œâ”€â”€ enrichment/
â”‚   â”œâ”€â”€ domain_context.ipynb
â”‚   â”œâ”€â”€ identity_context.ipynb
â”‚   â””â”€â”€ baseline_context.ipynb
â”‚
â”œâ”€â”€ detections/
â”‚   â”œâ”€â”€ phishing_suspected_sender.md
â”‚   â”œâ”€â”€ malicious_domain_first_seen.md
â”‚   â”œâ”€â”€ bulk_failed_logins.md
â”‚   â”œâ”€â”€ impossible_travel.md
â”‚   â”œâ”€â”€ anomalous_login_after_email.md
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ alerting/
â”‚   â”œâ”€â”€ alert_schema.json
â”‚   â”œâ”€â”€ severity_scoring.md
â”‚   â””â”€â”€ alert_gating.md
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ executive_overview.md
â”‚   â”œâ”€â”€ analyst_triage.md
â”‚   â””â”€â”€ investigation_view.md
â”‚
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ phishing_response.md
â”‚   â””â”€â”€ credential_compromise.md
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ soc_metrics.md
â”‚   â””â”€â”€ false_positive_stress_test.ipynb
â”‚
â””â”€â”€ cloud_mapping/
    â”œâ”€â”€ azure_sentinel_mapping.md
    â””â”€â”€ aws_opensearch_mapping.md
```

---

## ğŸ§ª Evaluation Philosophy

System effectiveness is evaluated using **SOC-relevant metrics**, not ML benchmarks:

* alerts per day
* false positives per day
* analyst workload estimates
* alert stability over time
* investigation completeness

This reflects real operational constraints rather than academic performance.

---

## â˜ï¸ Cloud Mapping

The architecture is intentionally **vendor-neutral**, with mappings provided for:

* Microsoft Sentinel (KQL-centric SIEM)
* AWS OpenSearch / Data Lake-centric pipelines

This allows the same detection and governance logic to be reasoned about across platforms.

---

## ğŸ§  Intended Audience

This project is designed for:

* SOC analysts and detection engineers
* security analytics engineers
* cloud security practitioners
* security architects
* interviewers evaluating real-world security thinking

It is **not** designed as a beginner tutorial or a data science demo.

---

## ğŸ Status

* Phase 1 â€” Schema & Contracts âœ…
* Phase 2 â€” Ingestion & Normalization â³
* Phase 3 â€” Context & Enrichment â³
* Phase 4 â€” Detection Engineering â³
* Phase 5 â€” Alerting & Governance â³
* Phase 6 â€” Dashboards & Investigation â³
* Phase 7 â€” Evaluation & Threat Modeling â³

Progress is intentionally incremental and review-driven.

---

## ğŸ” Final Note

This repository prioritizes **credibility over flash**.

The goal is not to impress with tools or models,
but to demonstrate how **real SOC systems are reasoned about, built, and defended**.
